# 2019CPC-
2019年国产CPU并行应用挑战赛优化代码

比赛时间对我们来说非常尴尬，我们报名的时候神威系统正在年检，年检完正好是我们三周的全天实训，所以代码写的很慌乱，望各位见谅。
下面对我们的代码做一些解释：
result文件夹下对应的四个cpc_exm*_buff是我们四个算例目前最好的结果，均可重新编译运行，编译指令直接执行make.sh即可，提交运行则直接运行run.sh，各个算例已经编译好了可执行文件，run.sh提交命令行也对应各个算例的要求。
cpc_exm1_mpi_buff文件夹下是我们对我们ppt最后描述的mpi进程级通信与主核心计算互相隐藏的一个尝试，（其余的文件夹中均实现了从核加速部分的数据流水双缓冲模式，即利用从核的计算时间掩盖掉了数据拷贝进入加速设备和拷贝出的时间，但未进行mpi通信与主核心的计算相互隐藏），编译和提交同上，参数和粒度还没来得及调整，所以效率不比其他算例高。

对于代码中还有一个未调好参数的设想，对于768阶的算例来说，MPI通信占了程序的大部分时间，在主核设置标志位，将主核通信对应于从核函数块的大小，以从核部分的计算和主核的计算掩盖下一次的主核MPI通信加从核DMA通信的跨级缓冲机制，
要求对MPI通信有粒度合适的划分，这个值我们目前还未找到。但是根据SWLU打桩分析的效果解决掉这块MPI通信的超高延迟会带来几倍的性能提升。


基准代码运行时间：算例一 190s 算例二：209s  算例三四 数据量太大无法运行 需要自己实现并行

目前状况：算例一 1.13s     算例二：0.38s      算例三：7.7s      算例四：1.93s

调试好如上通信隐藏后的预算时间
算例一：0.58s	算例二： 0.2s	算例三：2.0s	算例四：0.7s
